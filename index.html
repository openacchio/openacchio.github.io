<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Olivier Penacchio</title> <meta name="author" content="Olivier Penacchio"> <meta name="description" content="Data science, biological modelling, sensory ecology, vision."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_webpage_small.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://openacchio.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Olivier</span> Penacchio </h1> <p class="desc">Data science | biological modelling | sensory ecology | vision. <a href="https://www.uab.cat/ca/ciencies-computacio/recerca-ciencies-computacio" rel="external nofollow noopener" target="_blank">Universitat Autònoma de Barcelona, Computer Science Department</a> • <a target="_blank" href="https://www.st-andrews.ac.uk/psychology-neuroscience/" rel="external nofollow noopener">University of St Andrews, Psychology and Neuroscience</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/penacchio-w640-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/penacchio-w640-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/penacchio-w640-1400.webp"></source> <img src="/assets/img/penacchio-w640.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="penacchio-w640.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>op5 [guess] st-andrews.ac.uk</p> </div> </div> <div class="clearfix"> <p>Natural environments shape sensory systems during evolution and development. My core research interest is to understand how the tight relationship between natural environments and sensory systems affects perception and signalling. My research covers questions such as the evolution of warning signals (a class of signals used by animals to advertise their toxicity or unprofitability), or the effects of urban environments on the human visual system.</p> <p>I am very open to suggestions, new ideas, and collaborations. The best way to reach me is by email.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Feb 24, 2024</th> <td> Meta-learning in active inference, Commentary on Binz et al. (2023) accepted for publication in Behavioral and Brain Science. </td> </tr> <tr> <th scope="row">Nov 14, 2023</th> <td> Our paper <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1200661/full" rel="external nofollow noopener" target="_blank">‘A computational neuroscience framework for quantifying warning signals’</a> accepted for publication in Methods in Ecology and Evolution! We propose an objective and quantitative definition of warning signals based on how patterns generate population activity in a neural model of the brain of the receiver. </td> </tr> <tr> <th scope="row">Jun 27, 2023</th> <td> Our study <a href="https://www.dropbox.com/s/29et1fowhaozlm7/A%20mechanistic%20account%20of%20visual%20discomfort%20Penacchio%20et%20al%202023%20revised%20version%20accepted.pdf?dl=0" rel="external nofollow noopener" target="_blank">‘A mechanistic account of visual discomfort’</a> accepted for publication in Frontiers in Neuroscience. </td> </tr> <tr> <th scope="row">May 8, 2023</th> <td> Our review <a href="https://onlinelibrary.wiley.com/doi/10.1111/jeb.14192" rel="external nofollow noopener" target="_blank">‘The evolution and ecology of multiple antipredator defenses’</a> led by D.Kikuchi and A.Exnerová accepted for publication in Journal of Evolutionary Biology invited special issue. </td> </tr> <tr> <th scope="row">Mar 24, 2023</th> <td> Paper with Justin Yeager <a href="https://royalsocietypublishing.org/doi/full/10.1098/rspb.2023.0327" rel="external nofollow noopener" target="_blank">‘Outcomes of multifarious selection on the evolution of visual signals’</a> out in Proceedings of the Royal Society B. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/cover_proposal_Penacchio_et_al_MEE-23-05-306.R2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/cover_proposal_Penacchio_et_al_MEE-23-05-306.R2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/cover_proposal_Penacchio_et_al_MEE-23-05-306.R2-1400.webp"></source> <img src="/assets/img/publication_preview/cover_proposal_Penacchio_et_al_MEE-23-05-306.R2.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cover_proposal_Penacchio_et_al_MEE-23-05-306.R2.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="WOS:001121340900001" class="col-sm-8"> <div class="title">A computational neuroscience framework for quantifying warning signals</div> <div class="author"> O. Penacchio, C. G. Halpin, I. C. Cuthill, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'P. G. Lovell, M. Wheelwright, J. Skelhorn, C. Rowe, J. M. Harris' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Methods in Ecology and Evolution</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.14268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://besjournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/2041-210X.14268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://besjournals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111%2F2041-210X.14268&amp;file=mee314268-sup-0001-Supinfo.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://doi.org/10.5061/dryad.x3ffbg7kd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1111/2041-210X.14268" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1111/2041-210X.14268" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Animal warning signals show remarkable diversity, yet subjectively appear to share certain visual features that make defended prey stand out and look different from more cryptic palatable species. For example, many (but far from all) warning signals involve high contrast elements, such as stripes and spots, and often involve the colours yellow and red. How exactly do aposematic species differ from non-aposematic ones in the eyes (and brains) of their predators? Here, we develop a novel computational modelling approach, to quantify prey warning signals and establish what visual features they share. First, we develop a model visual system, made of artificial neurons with realistic receptive fields, to provide a quantitative estimate of the neural activity in the first stages of the visual system of a predator in response to a pattern. The system can be tailored to specific species. Second, we build a novel model that defines a ‘neural signature’, comprising quantitative metrics that measure the strength of stimulation of the population of neurons in response to patterns. This framework allows us to test how individual patterns stimulate the model predator visual system. For the predator-prey system of birds foraging on lepidopteran prey, we compared the strength of stimulation of a modelled avian visual system in response to a novel database of hyperspectral images of aposematic and undefended butterflies and moths. Warning signals generate significantly stronger activity in the model visual system, setting them apart from the patterns of undefended species. The activity was also very different from that seen in response to natural scenes. Therefore, to their predators, lepidopteran warning patterns are distinct from their non-defended counterparts and stand out against a range of natural backgrounds. For the first time, we present an objective and quantitative definition of warning signals based on how the pattern generates population activity in a neural model of the brain of the receiver. This opens new perspectives for understanding and testing how warning signals have evolved, and, more generally, how sensory systems constrain signal design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WOS:001121340900001</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Penacchio, O. and Halpin, C. G. and Cuthill, I. C. and Lovell, P. G. and Wheelwright, M. and Skelhorn, J. and Rowe, C. and Harris, J. M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A computational neuroscience framework for quantifying warning signals}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Methods in Ecology and Evolution}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103-116}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/2041-210X.14268}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2041-210X}</span><span class="p">,</span>
  <span class="na">eissn</span> <span class="p">=</span> <span class="s">{2041-2096}</span><span class="p">,</span>
  <span class="na">osf</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5061/dryad.x3ffbg7kd}</span><span class="p">,</span>
  <span class="na">database</span> <span class="p">=</span> <span class="s">{https://arts.st-andrews.ac.uk/lepidoptera/}</span><span class="p">,</span>
  <span class="na">orcid-numbers</span> <span class="p">=</span> <span class="s">{Cuthill, Innes/0000-0002-5007-8856
     Harris, Julie/0000-0002-3497-4503
     Skelhorn, John/0000-0002-8385-4831
     Lovell, Paul George/0000-0003-2959-5370
     Rowe, Candy/0000-0001-5379-843X}</span><span class="p">,</span>
  <span class="na">unique-id</span> <span class="p">=</span> <span class="s">{WOS:001121340900001}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202023-1400.webp"></source> <img src="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202023.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="illustration Penacchio et al Frontiers in Neuroscience 2023.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="WOS:001041614900001" class="col-sm-8"> <div class="title">A mechanistic account of visual discomfort</div> <div class="author"> <em>Olivier Penacchio</em>, Xavier Otazu, Arnold J. Wilkins, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sarah M. Haigh' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1200661/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/A%20mechanistic%20account%20of%20visual%20discomfort%20Penacchio%20et%20al%202023%20revised%20version%20accepted.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/Supplementary%20Material%20A%20mechanistic%20account%20of%20visual%20discomfort%20Penacchio%20et%20al%202023%20revised%20version%20accepted.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://doi.org/10.5061/dryad.g79cnp5kw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="tbd" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="tbd" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Much of the neural machinery of the early visual cortex, from the extraction of local orientations to contextual modulations through lateral interactions, is thought to have developed to provide a sparse encoding of contour in natural scenes, allowing the brain to process efficiently most of the visual scenes we are exposed to. Certain visual stimuli, however, cause visual stress, a set of adverse effects ranging from simple discomfort to migraine attacks, and epileptic seizures in the extreme, all phenomena linked with an excessive metabolic demand. The theory of efficient coding suggests a link between excessive metabolic demand and images that deviate from natural statistics. Yet, the mechanisms linking energy demand and image spatial content in discomfort remain elusive. Here, we used theories of visual coding that link image spatial structure and brain activation to characterize the response to images observers reported as uncomfortable in a biologically based neurodynamic model of the early visual cortex that included excitatory and inhibitory layers to implement contextual influences. We found three clear markers of aversive images: a larger overall activation in the model, a less sparse response, and a more unbalanced distribution of activity across spatial orientations. When the ratio of excitation over inhibition was increased in the model, a phenomenon hypothesised to underlie interindividual differences in susceptibility to visual discomfort, the three markers of discomfort progressively shifted towards values typical of the response to uncomfortable stimuli. Overall, these findings propose a unifying mechanistic explanation for why there are differences between images and between observers, suggesting how visual input and idiosyncratic hyperexcitability give rise to abnormal brain responses that result in visual stress.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WOS:001041614900001</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Penacchio, Olivier and Otazu, Xavier and Wilkins, Arnold J. and Haigh, Sarah M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A mechanistic account of visual discomfort}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{tbd}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{tbd}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{tbd}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{tbd}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://doi.org/10.5061/dryad.g79cnp5kw}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/animation_Yeager_Penacchio_ProcRoySocB_2023.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/animation_Yeager_Penacchio_ProcRoySocB_2023.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/animation_Yeager_Penacchio_ProcRoySocB_2023.gif-1400.webp"></source> <img src="/assets/img/publication_preview/animation_Yeager_Penacchio_ProcRoySocB_2023.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="animation_Yeager_Penacchio_ProcRoySocB_2023.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1098/rspb.2023.0327" class="col-sm-8"> <div class="title">Outcomes of multifarious selection on the evolution of visual signals</div> <div class="author"> <a href="http://www.justinyeager.org/" rel="external nofollow noopener" target="_blank">Justin Yeager*</a>, and <em>Olivier Penacchio*</em> </div> <div class="periodical"> <em>Proceedings of the Royal Society B</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://royalsocietypublishing.org/doi/full/10.1098/rspb.2023.0327" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Outcomes%20of%20multifarious%20selection%20on%20the%20evolution%20of%20visual%20signals%20Yeager%20Penacchio%20Proc%20Roy%20Soc%20B%202023%20accepted%20version.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1098/rspb.2023.0327" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1098/rspb.2023.0327" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Multifarious sources of selection shape visual signals and can produce phenotypic divergence. Theory predicts that variance in warning signals should be minimal due to purifying selection, yet polymorphism is abundant. While in some instances divergent signals can evolve into discrete morphs, continuously variable phenotypes are also encountered in natural populations. Notwithstanding, we currently have an incomplete understanding of how combinations of selection shape fitness landscapes, particularly those which produce polymorphism. We modeled how combinations of natural and sexual selection act on aposematic traits within a single population to gain insights into what combinations of selection favors the evolution and maintenance of phenotypic variation. With a rich foundation of studies on selection and phenotypic divergence, we reference the poison frog genus Oophaga to model signal evolution. Multifarious selection on aposematic traits created the topology of our model’s fitness landscape by approximating different scenarios found in natural populations. Combined, the model produced all types of phenotypic variation found in frog populations, namely monomorphism, continuous variation, and discrete polymorphism. Our results afford advances into how multifarious selection shapes phenotypic divergence, which, along with additional modelling enhancements, will allow us to further our understanding of visual signal evolution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202021.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202021.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202021.JPG-1400.webp"></source> <img src="/assets/img/publication_preview/illustration%20Penacchio%20et%20al%20Frontiers%20in%20Neuroscience%202021.JPG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="illustration Penacchio et al Frontiers in Neuroscience 2021.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.3389/fnins.2021.711066" class="col-sm-8"> <div class="title">Visual Discomfort and Variations in Chromaticity in Art and Nature</div> <div class="author"> <em>Olivier Penacchio</em>, Sarah M. Haigh, Xortia Ross, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Rebecca Ferguson, Arnold J. Wilkins' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2021.711064/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Visual%20Discomfort%20and%20Variations%20in%20Chromaticity%20Frontiers%20in%20Neuroscience%20Penacchio%20et%20al%202021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/Suppl%20Mat%20Visual%20Discomfort%20and%20Variations%20in%20Chromaticity%20Frontiers%20in%20Neuroscience%20Penacchio%20et%20al%202021.pdf" class="btn btn-sm z-depth-0" role="button">Supp</a> <a href="https://github.com/openacchio/polymorphism-scenarios-and-free-energy-solver" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.3389/fnins.2021.711064" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.3389/fnins.2021.711064" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Visual discomfort is related to the statistical regularity of visual images. The contribution of luminance contrast to visual discomfort is well understood and can be framed in terms of a theory of efficient coding of natural stimuli, and linked to metabolic demand. While color is important in our interaction with nature, the effect of color on visual discomfort has received less attention. In this study, we build on the established association between visual discomfort and differences in chromaticity across space. We average the local differences in chromaticity in an image and show that this average is a good predictor of visual discomfort from the image. It accounts for part of the variance left unexplained by variations in luminance. We show that the local chromaticity difference in uncomfortable stimuli is high compared to that typical in natural scenes, except in particular infrequent conditions such as the arrangement of colorful fruits against foliage. Overall, our study discloses a new link between visual ecology and discomfort whereby discomfort arises when adaptive perceptual mechanisms are overstimulated by specific classes of stimuli rarely found in nature.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.3389/fnins.2021.711066</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Penacchio, Olivier and Haigh, Sarah M. and Ross, Xortia and Ferguson, Rebecca and Wilkins, Arnold J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Discomfort and Variations in Chromaticity in Art and Nature}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fnins.2021.711064}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1662-453X}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.3389/fnins.2021.711064}</span><span class="p">,</span>
  <span class="na">data</span> <span class="p">=</span> <span class="s">{https://github.com/openacchio/polymorphism-scenarios-and-free-energy-solver}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/illustration%20Cuthill%20et%20al%20PNAS%202016.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/illustration%20Cuthill%20et%20al%20PNAS%202016.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/illustration%20Cuthill%20et%20al%20PNAS%202016.JPG-1400.webp"></source> <img src="/assets/img/publication_preview/illustration%20Cuthill%20et%20al%20PNAS%202016.JPG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="illustration Cuthill et al PNAS 2016.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="WOS:000388970100065" class="col-sm-8"> <div class="title">Optimizing countershading camouflage</div> <div class="author"> Innes C. Cuthill, N. Simon Sanghera, <em>Olivier Penacchio</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Paul George Lovell, Graeme D. Ruxton, Julie M. Harris' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences of the United States of America</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.pnas.org/doi/full/10.1073/pnas.1611589113" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Optimizing%20countershading%20camouflage%20Cuthill%20et%20al%20PNAS%202016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1073/pnas.1611589113" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1073/pnas.1611589113" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Countershading, the widespread tendency of animals to be darker on the side that receives strongest illumination, has classically been explained as an adaptation for camouflage: obliterating cues to 3D shape and enhancing background matching. However, there have only been two quantitative tests of whether the patterns observed in different species match the optimal shading to obliterate 3D cues, and no tests of whether optimal countershading actually improves concealment or survival. We use a mathematical model of the light field to predict the optimal countershading for concealment that is specific to the light environment and then test this prediction with correspondingly patterned model “caterpillars” exposed to avian predation in the field. We show that the optimal countershading is strongly illumination-dependent. A relatively sharp transition in surface patterning from dark to light is only optimal under direct solar illumination; if there is diffuse illumination from cloudy skies or shade, the pattern provides no advantage over homogeneous background-matching coloration. Conversely, a smoother gradation between dark and light is optimal under cloudy skies or shade. The demonstration of these illumination-dependent effects of different countershading patterns on predation risk strongly supports the comparative evidence showing that the type of countershading varies with light environment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WOS:000388970100065</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cuthill, Innes C. and Sanghera, N. Simon and Penacchio, Olivier and Lovell, Paul George and Ruxton, Graeme D. and Harris, Julie M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimizing countershading camouflage}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the National Academy of Sciences of the United States of America}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{113}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{46}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13093-13097}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1073/pnas.1611589113}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0027-8424}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1073/pnas.1611589113}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20Wilkins%20Vision%20Research%202015.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20Wilkins%20Vision%20Research%202015.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20Wilkins%20Vision%20Research%202015.JPG-1400.webp"></source> <img src="/assets/img/publication_preview/illustration%20Penacchio%20Wilkins%20Vision%20Research%202015.JPG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="illustration Penacchio Wilkins Vision Research 2015.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="WOS:000350781100001" class="col-sm-8"> <div class="title">Visual discomfort and the spatial distribution of Fourier energy</div> <div class="author"> <em>Olivier Penacchio</em>, and Arnold J. Wilkins</div> <div class="periodical"> <em>Vision Research</em>, 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0042698914003320?via%3Dihub" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/visual%20discomfort%20and%20the%20spatial%20distribution%20of%20Fourier%20energy%20Penacchio%20Wilkins%20VR%202015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1016/j.visres.2014.12.013" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.visres.2014.12.013" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Quite independently of what they represent, some images provoke discomfort, and even headaches and seizures in susceptible individuals. The visual system has adapted to efficiently process the images it typically experiences, and in nature these images are usually scale-invariant. In this work, we sought to characterize the images responsible for discomfort in terms of their adherence to low-level statistical properties typically seen in natural scenes. It has been conventional to measure scale invariance in terms of the one-dimensional Fourier amplitude spectrum, by averaging amplitude over orientations in the Fourier domain. However, this loses information on the evenness with which information at various orientations is represented. We therefore fitted a two-dimensional surface (regular circular cone 1/f in logarithmic coordinates) to the two-dimensional amplitude spectrum. The extent to which the cone fitted the spectrum explained an average of 18% of the variance in judgments of discomfort from images including rural and urban scenes, works of non-representational art, images of buildings and animals, and images generated from randomly disposed discs of varying contrast and size. Weighting the spectrum prior to fitting the surface to allow for the spatial frequency tuning of contrast sensitivity explained an average of 27% of the variance. Adjusting the shape of the cone to take account of the generally greater energy in horizontal and vertical orientations improved the fit, but only slightly. Taken together, our findings show that a simple measure based on first principles of efficient coding and human visual sensitivity explained more variance than previously published algorithms. The algorithm has a low computational cost and we show that it can identify the images involved in cases that have reached the media because of complaints. We offer the algorithm as a tool for designers rather than as a simulation of the biological processes involved. (C) 2014 Elsevier Ltd. All rights reserved.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WOS:000350781100001</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Penacchio, Olivier and Wilkins, Arnold J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual discomfort and the spatial distribution of Fourier energy}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Vision Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{108}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-7}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.visres.2014.12.013}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0042-6989}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.visres.2014.12.013}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20at%20al%20American%20Naturalist%202015.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20at%20al%20American%20Naturalist%202015.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/illustration%20Penacchio%20at%20al%20American%20Naturalist%202015.JPG-1400.webp"></source> <img src="/assets/img/publication_preview/illustration%20Penacchio%20at%20al%20American%20Naturalist%202015.JPG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="illustration Penacchio at al American Naturalist 2015.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="WOS:000362840100014" class="col-sm-8"> <div class="title">Three-Dimensional Camouflage: Exploiting Photons to Conceal Form</div> <div class="author"> <em>Olivier Penacchio</em>, P. George Lovell, Innes C. Cuthill, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Graeme D. Ruxton, Julie M. Harris' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>American Naturalist</em>, 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.journals.uchicago.edu/doi/10.1086/682570" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Three-dimensional%20camouflage%20Exploiting%20photons%20Penacchio%20et%20al%20Am.Nat.%202015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1086/682570" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1086/682570" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Many animals have a gradation of body color, termed countershading, where the areas that are typically exposed to more light are darker. One hypothesis is that this patterning enhances visual camouflage by making the retinal image of the animal match that of the background, a fundamentally two-dimensional theory. More controversially, countershading may also obliterate cues to three-dimensional (3D) shape delivered by shading. Despite relying on distinct cognitive mechanisms, these two potential functions hitherto have been amalgamated in the literature. It has previously not been possible to validate either hypothesis empirically, because there has been no general theory of optimal countershading that allows quantitative predictions to be made about the many environmental parameters involved. Here we unpack the logical distinction between using countershading for background matching and using it to obliterate 3D shape. We use computational modeling to determine the optimal coloration for the camouflage of 3D shape. Our model of 3D concealment is derived from the physics of light and informed by perceptual psychology: we simulate a 3D world that incorporates naturalistic lighting environments. The model allows us to predict countershading coloration for terrestrial environments, for any body shape and a wide range of ecologically relevant parameters. The approach can be generalized to any light distribution, including those underwater.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">WOS:000362840100014</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Penacchio, Olivier and Lovell, P. George and Cuthill, Innes C. and Ruxton, Graeme D. and Harris, Julie M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Three-Dimensional Camouflage: Exploiting Photons to Conceal Form}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{American Naturalist}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{186}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{553-563}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1086/682570}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0003-0147}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1086/682570}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6F%70%65%6E%61%63%63%68%69%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-1544-2405" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=z07K3hoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/openacchio" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> The best way to reach me is by email. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Olivier Penacchio. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>